{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas de NLTK para NLP.\n",
    "\n",
    "## Rumbo a un organizador de archivos.\n",
    "\n",
    "**Algunas hipótesis:** lo conveniente para tratar un texto es separarlo en sus partes fundamentales, es decir sus subconjuntos determinados por los indicadores codificádos en el texto. Es decir, un documento se divide en:\n",
    "- **Archivo:** Marcado por el código ascii de EOF (En of file), este determina que el archivo tenga identidad.\n",
    "- **Parrafo:** Este debe es marcado por el salto de linea ( **\\n** )\n",
    "- **Fin de página:** De existir en el tipo de fichero.\n",
    "- **Capitulo o temas**: De existir, debe ir acompañado de números iniciales o decir explicitamente capitulo (en su idioma correspondiente.\n",
    "- **Palabras**\n",
    "- **Letras**\n",
    "- **Signos de puntuación**\n",
    "\n",
    "$$ A = \\{\\{A,B,\\{\\}\\},\\{C\\},D\\} $$\n",
    "\n",
    "## Organización de una factura\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing package\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import gensim\n",
    "import PyPDF2 \n",
    "import textract\n",
    "# importing package functions \n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.book import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir(gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<open file '/home/sistemas/Compras/Pedidos Mayoristas/Pedidos Mayoristas 2017/Makicop/01 Enero/Makicop 289 Toner Sharp 2610 31Ene17/TR 15957 FACT. M 8961 MAKICOP.pdf', mode 'rb' at 0x7f0f8b30da50>\n",
      "<PyPDF2.pdf.PdfFileReader object at 0x7f0f8afffe50>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Declaración de la ruta\n",
    "path = '/home/sistemas/Documentos/Recuperados/pdf/Eliminar/Facturas/'\n",
    "#name = path+'f1558602112.pdf'\n",
    "name=\"/home/sistemas/\"\n",
    "name+=\"Compras/Pedidos Mayoristas/Pedidos Mayoristas 2017/Makicop\"\n",
    "name+=\"/01 Enero/Makicop 289 Toner Sharp 2610 31Ene17/\"\n",
    "name+=\"TR 15957 FACT. M 8961 MAKICOP.pdf\"\n",
    "name2 = path+'f1352038192.pdf'\n",
    "# Abrir el PDF\n",
    "pdfFileObj = open(name,'rb')\n",
    "print pdfFileObj\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "print pdfReader\n",
    "num_pages = pdfReader.numPages\n",
    "print num_pages\n",
    "count = 0\n",
    "text = \"\\\"\"\n",
    "raw_documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nif text[0] != \"\":\\n   text = text\\nelse:\\n   text = textract.process(fileurl, method=\\'tesseract\\', language=\\'es\\')\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for dirname, dirnames, filenames in os.walk('/home/sistemas/Imágenes'):\n",
    "#    for filename in filenames:\n",
    "#        print(os.path.join(dirname, filename))\n",
    "\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    print pageObj\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    print text\n",
    "\n",
    "text+=\"\\\"\"    \n",
    "raw_documents.append(text)\n",
    "    \n",
    "print text\n",
    "\"\"\"\n",
    "if text[0] != \"\":\n",
    "   text = text\n",
    "else:\n",
    "   text = textract.process(fileurl, method='tesseract', language='es')\n",
    "\"\"\"\n",
    "#text = [text]\n",
    "#print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print raw_documents\n",
    "#print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_documents]\n",
    "# print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "#print(dictionary[5])\n",
    "# print(dictionary.token2id['road'])\n",
    "#print(\"Number of words in dictionary:\",len(dictionary))\n",
    "#for i in range(len(dictionary)):\n",
    "#    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "# print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "#print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 1 documents in 0 shards (stored under /home/sistemas/Documentos/Scripts/)\n"
     ]
    }
   ],
   "source": [
    "sims = gensim.similarities.Similarity('/home/sistemas/Documentos/Scripts/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "# print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir el PDF\n",
    "pdfFileObj2 = open(name2,'rb')\n",
    "pdfReader2 = PyPDF2.PdfFileReader(pdfFileObj2)\n",
    "num_pages = pdfReader2.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "raw_query = []\n",
    "\n",
    "while count < num_pages:\n",
    "    pageObj2 = pdfReader2.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj2.extractText()\n",
    "\n",
    "#text+=\"\\\"\"    \n",
    "#raw_query.append(text)\n",
    "    \n",
    "#print text\n",
    "\n",
    "###\n",
    "query_doc = [w.lower() for w in word_tokenize(text)] \n",
    "# query_doc = [w.lower() for w in word_tokenize(\"Socks are a force for good.\")]\n",
    "#print(query_doc)\n",
    "\n",
    "####\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "#print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "#print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nword_tokenize\\ngen_docs = [[w.lower() for w in word_tokenize(textit)] \\n            for textit in text]\\nprint(gen_docs)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(textit)] \n",
    "            for textit in text]\n",
    "print(gen_docs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'SEBASTIAN', u'DEL', u'PIOMBO', u',', u'82-2', u',', u',', u'NONOALCO', u',', u',', u'BENITO', u'JUAREZ', u',', u'CIUDAD', u'DE', u'MEXICO', u',', u'MEXICO', u',', u'C.P', u'.', u'03700', u',', u'Tel', u'56832154', u',', u'56839289EDS0508123K0', u'208421/12/2016Dirigido', u'a', u':', u'VENTAS', u'Cliente', u':', u'ConfirmadoFOLIOFECHAESTATUS', u'INFORMACI\\xd3N', u'DEL', u'CLIENTE', u'2,938', u'COMERCIALIZADORA', u'DE', u'LACTEO', u'Direcci\\xf3n', u':', u'LAZARO', u'CARDENAS', u'185', u',', u'PARQUE', u'INDUSTRIAL', u'LAGUNERO', u',', u',', u'Durango', u',', u'GOMEZ', u'PALACIO', u',', u'MEXICORFC', u':', u'CLD0507145H6PEDIDOEAO', u'DIGITAL', u'SOLUTIONS', u'S.A.', u'DE', u'C.V.', u'PESOSAGENTE', u'DE', u'VENTAS', u'LETICIA', u'FLORES', u'DE', u'LA', u'CRUZ', u'INFORMACI\\xd3N', u'DEL', u'PAGO', u'M\\xe9todo', u'pago:03', u'TRANSFERENCIA', u'ELECTRONICA', u'DE', u'FONDOSForma', u'pago:75', u'DIASINFORMACI\\xd3N', u'DEL', u'EQUIPO', u'ClaveDescripci\\xf3nCant.P', u'.', u'U.SubtotalTotal', u'AlmacenZonSec.AnaqueEspacioNombre', u':', u'MONEDA1T02L10US0KYOCERA', u'CARTUCHO', u'DE', u'TONER', u'NEGRO', u'TK', u'-3122', u'$', u'1,589.00PRINCIPAL', u'33', u'$', u'60,826.92', u'$', u'52,437.00ENVIOENVIO', u'DE', u'MERCANCIA', u'$', u'0.00PRINCIPAL', u'1', u'$', u'0.00', u'$', u'0.00TOTAL', u'I.V.A.DESC.SUB-TOTAL', u'16', u'%', u'$', u'52,437.00', u'0.00', u'$', u'60,826.92', u'$', u'8,389.92Importe', u'con', u'Letras', u':', u'SESENTA', u'MIL', u'OCHOCIENTOS', u'VEINTISEIS', u'PESOS', u'92/100', u'MXN', u'NOTA', u':', u'OC', u'-', u'4505018919', u',', u'ENTREGA', u'AV', u'.', u'ABRAHAM', u'LINCOLN', u'4917', u',', u'FRACC', u'.', u'VILLA', u'MITRAS', u'MONTERREY', u',', u'N.L', u'.', u'C.P', u'.', u'64170AT\\xb4N', u';', u'JOSE', u'LUIS', u'MONTOYA', u'/', u'EDGAR', u'RAMIREZ', u'Y', u'/O', u'ERNESTO', u'RODRIGUEZ', u'21/12/201601:33:17p.m.Page', u'1', u'of', u'1Fecha', u'y', u'hora', u'de', u'impresion', u':']\n"
     ]
    }
   ],
   "source": [
    "#print num_pages\n",
    "#print text\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "stop_words = stopwords.words('spanish')\n",
    "\"\"\"\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:])\n",
    "\"\"\"\n",
    "#print tokens\n",
    "text = nltk.Text(tokens)\n",
    "print tokens\n",
    "#text.findall(\"<Sebastian>(<.*>)<del>\")\n",
    "#dictionary=gensim.corpora.Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEXICO\n",
      "MEXICO\n",
      "MEXICORFC\n"
     ]
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    if \"MEXICO\" in i:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print sys.path\n",
    "import matplotlib.pyplot as plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'[',\n",
       " u'Moby',\n",
       " u'Dick',\n",
       " u'by',\n",
       " u'Herman',\n",
       " u'Melville',\n",
       " u'1851',\n",
       " u']',\n",
       " u'ETYMOLOGY']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from nltk.book import *\n",
    "text1\n",
    "text1[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "532748\n",
      "﻿The Project Gutenberg EBook of Juvenilla; Prosa ligera, by Miguel Cané\r\n",
      "\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "#import urllib\n",
    "\"\"\"\n",
    "Descarga de un texto\n",
    "\"\"\"\n",
    "url = \"http://www.gutenberg.org/cache/epub/41575/pg41575.txt\"\n",
    "\n",
    "response = urllib2.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "101107\n",
      "[u'\\ufeffThe', u'Project', u'Gutenberg', u'EBook', u'of', u'Juvenilla', u';', u'Prosa', u'ligera', u',']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "101107\n",
      "[u'\\ufeffThe', u'Project', u'Gutenberg', u'EBook', u'of', u'Juvenilla', u';', u'Prosa', u'ligera', u',']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sabor; subjetivismo; aparato; momento; sentimiento; orden; acento;\n",
      "problema\n"
     ]
    }
   ],
   "source": [
    "text.findall(\"<un>(<.*>)<especial>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 25 matches:\n",
      "e fué esa época de la actual , que de ella sólo queda el recuerdo , formado , pa\n",
      "s que él hiciera en su favor . Aunque ella , más que una solución , -- la Facult\n",
      "vedad que aleja la desconfianza y con ella la defensa de sí misma [ 1 ] . Entonc\n",
      ", la impersonalidad , entendiendo por ella la facultad de dominar las simpatías \n",
      "ecto era a mis ojos no ver figurar en ella a D'Artagnan , principal personaje de\n",
      " nos fuimos arrodillando y posando en ella los labios , como un adiós supremo a \n",
      " pulso hasta su tumba y levantamos en ella un modesto monumento con nuestros pob\n",
      "aumento , que era necesario alojar en ella . Una epidemia vaga , indefinida , ha\n",
      "raba indecibles delicias . Cargué con ella y cuando bajé los ojos para buscar ot\n",
      "ó . XXXI Pero la juventud venía y con ella todas las aspiraciones indefinibles. \n",
      "a manera . He tenido gran afición por ella , afición que , con los años , ya pas\n",
      "o nos penetraba hasta los huesos . En ella hicimos campamento , pues , en democr\n",
      "s ha sido venerada en toda España . A ella enviaba reverente don Juan de Austria\n",
      "n todas sus manifestaciones . Todo en ella venía de Dios y todo volvía a Dios , \n",
      "y que creemos sincero , fuera hijo de ella , sentiría en el alma algo instintivo\n",
      "e . A veces cuando el sol vibra sobre ella con tal intensidad que el suelo se en\n",
      "ersal de todo lo que siente y sufre ; ella sola puede traducir con la vaguedad n\n",
      "lón , tan llorada por todos los que a ella teníamos vinculada nuestra juventud y\n",
      " , la vida corría y las tristezas con ella . Estaba solo en cubierta , tendido s\n",
      "ad y eran capaces de sacrificarse por ella . No se hablaba de otra cosa ; los di\n",
      " tarjeta . Apenas echó los ojos sobre ella , sintió una emoción violenta , se pu\n",
      " la proscripción , puesto que vivo en ella hace 20 años . Respeto el móvil de mi\n",
      "va que se le manifestó , si es que de ella se apercibió , no le hizo la menor im\n",
      "iz aquel que consigue desprenderse de ella antes que sus facultades se hayan cri\n",
      "ble , si es que su nombre llega hasta ella . Las acciones de Bacon se han de cot\n"
     ]
    }
   ],
   "source": [
    "text.concordance(\"ella\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg-tm; Buenos Aires; todos los; Project Gutenberg; más\n",
      "tarde; todas las; Sr. Abeille; Literary Archive; tal vez; Gutenberg-tm\n",
      "electronic; electronic works; Gutenberg Literary; San Martín; Archive\n",
      "Foundation; sin duda; United States; los ojos; primera vez; los\n",
      "hombres; día siguiente; Por fin; cada instante; sin embargo; sus ojos;\n",
      "Guzmán Blanco; set forth; las cosas; señor ministro; nuestro país;\n",
      "cinco años; public domain; con una; Colegio Nacional; electronic work;\n",
      "los primeros; que nos; Estados Unidos; PROSA LIGERA; había hecho; una\n",
      "sola; Miguel Cané; entre nosotros; doctor Agüero; los campos; por\n",
      "cierto; otra parte; Gutenberg-tm License; Sáenz Peña; los últimos;\n",
      "sobre todo; del Colegio; todas partes; mayor parte; tres meses; que\n",
      "había; _Don Juan_; medida que; San Ignacio; mis compañeros; ilustre\n",
      "americano; muchas veces; Prosa Ligera; entre los; han sido; señor\n",
      "presidente; idioma nacional; había sido; largos años; Relaciones\n",
      "Exteriores; _El Nacional_; relaciones exteriores; nuestros padres;\n",
      "media hora; los claustros; las manos; estas líneas; coronel Galindo;\n",
      "las ideas; siglo XVII; cepa criolla; Más tarde; por primera; darse\n",
      "cuenta; nos pusimos; del arte; Luis Felipe; una palabra; veinte años;\n",
      "todo eso; mis ojos; por completo; las primeras; estas páginas; para\n",
      "nosotros; los pueblos; nuestra tierra; copyright holder; largas horas;\n",
      "nuestra lengua; las horas\n"
     ]
    }
   ],
   "source": [
    "text.collocations(num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'degenera',\n",
       " u'pinchazos',\n",
       " u'sentir\\xe9is',\n",
       " u'emancipa',\n",
       " u'escriba',\n",
       " u'dej\\xe1bamos',\n",
       " u'conjura',\n",
       " u'impotente',\n",
       " u'trotar',\n",
       " u'accidentada',\n",
       " u'Comprend\\xeda',\n",
       " u'olvid\\xf3',\n",
       " u'alimentado',\n",
       " u't\\xeda',\n",
       " u'est\\xe9riles',\n",
       " u'Urquiza',\n",
       " u'inmaculado',\n",
       " u'soledades',\n",
       " u'compuesta',\n",
       " u'compuesto']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fdist1 = FreqDist(text)\n",
    "fdist1.hapaxes()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'cuando', 125), (u'hombre', 117), (u'esp\\xedritu', 108), (u'tiempo', 107), (u'porque', 103), (u'momento', 96), (u'siempre', 95), (u'hombres', 87), (u'Project', 83), (u'nosotros', 82), (u'nuestra', 81), (u'Sarmiento', 78), (u'Carlos', 76), (u'tierra', 76), (u'entonces', 70), (u'nuestro', 69), (u'despu\\xe9s', 69), (u'contra', 62), (u'hab\\xedan', 61), (u'nuestros', 61), (u'estaba', 59), (u'historia', 57), (u'Gutenberg-tm', 55), (u'tambi\\xe9n', 53), (u'primera', 53), (u'nombre', 52), (u'Colegio', 50), (u'frente', 50), (u'manera', 50), (u'aquella', 48), (u'habr\\xeda', 48), (u'Espa\\xf1a', 48), (u'palabra', 47), (u'algunos', 46), (u'cuerpo', 45), (u'Jacques', 45), (u'memoria', 44), (u'pronto', 44), (u'pasado', 43), (u'cabeza', 42), (u'primer', 42), (u'pueblo', 42), (u'patria', 41), (u'puerta', 40), (u'general', 40), (u'durante', 39), (u'mientras', 39), (u'acci\\xf3n', 38), (u'hubiera', 38), (u'pol\\xedtica', 38), (u'Buenos', 37), (u'recuerdo', 37), (u'hablar', 37), (u'movimiento', 36), (u'aquellos', 35), (u'cierto', 35), (u'impresi\\xf3n', 34), (u'porvenir', 34), (u'camino', 34), (u'Europa', 34), (u'fuerza', 34), (u'esfuerzo', 34), (u'puesto', 33), (u'intelectual', 33), (u'coraz\\xf3n', 33), (u'marcha', 33), (u'ma\\xf1ana', 31), (u'estado', 31), (u'car\\xe1cter', 31), (u'atm\\xf3sfera', 31), (u'dentro', 31), (u'cuenta', 31), (u'muchos', 31), (u'compa\\xf1eros', 30), (u'nuestras', 30), (u'completo', 29), (u'p\\xe1ginas', 29), (u'sociedad', 29), (u'inteligencia', 29), (u'humana', 29), (u'enorme', 29), (u'franc\\xe9s', 29), (u'Cuando', 29), (u'gobierno', 28), (u'llegar', 28), (u'j\\xf3venes', 28), (u'espa\\xf1ol', 28), (u'ministro', 28), (u'necesario', 28), (u'juventud', 28), (u'naturaleza', 28), (u'verdad', 27), (u'social', 27), (u'profunda', 27), (u'electronic', 27), (u'Gutenberg', 27), (u'caballo', 27), (u'algunas', 27), (u'Larrea', 27), (u'pensamiento', 26), (u'cuatro', 26), (u'siguiente', 26), (u'Montevideo', 26), (u'conciencia', 26), (u'cari\\xf1o', 25), (u'cuanto', 25), (u'aspecto', 25), (u'libros', 25), (u'primero', 25), (u'tenido', 25), (u'familia', 25), (u'Foundation', 24), (u'Francia', 24), (u'silencio', 24), (u'sentido', 24), (u'mirada', 24), (u'Benito', 24), (u'primeros', 24), (u'grandes', 24), (u'Castellar', 23), (u'nacional', 23), (u'parec\\xeda', 23), (u'Narbal', 23), (u'recuerdos', 23), (u'Eyzaguirre', 23), (u'triste', 23), (u'sangre', 23), (u'amigos', 23), (u'americano', 23), (u'instante', 23), (u'espa\\xf1ola', 22), (u'entera', 22), (u'tiempos', 22), (u'estaban', 22), (u'veinte', 22), (u'estudio', 22), (u'lengua', 22), (u'm\\xfasica', 22), (u'momentos', 22), (u'cuarto', 22), (u'idioma', 22), (u'Am\\xe9rica', 22), (u'posible', 21), (u'ciencia', 21), (u'Lorenzo', 21), (u'estilo', 21), (u'embargo', 21), (u'\\xfaltimo', 21), (u'libertad', 21), (u'parece', 21), (u'expresi\\xf3n', 21), (u'encontraba', 21), (u'influencia', 21), (u'natural', 20), (u'Corrales', 20), (u'muerte', 20), (u'argentinos', 20), (u'trabajo', 20), (u'admirable', 19), (u'perros', 19), (u'cuadro', 19), (u'doctor', 19), (u'presidente', 19), (u'aunque', 19), (u'derecho', 19), (u'revoluci\\xf3n', 19), (u'fueron', 19), (u'Felipe', 18), (u'constante', 18), (u'campa\\xf1a', 18), (u'medida', 18), (u'escrito', 18), (u'art\\xedculo', 18), (u'perdido', 18), (u'placer', 18), (u'Abeille', 18), (u'campos', 18), (u'diplom\\xe1tico', 18), (u'r\\xe9gimen', 18), (u'actual', 18), (u'Rejalte', 18), (u'aquellas', 18), (u'ten\\xedan', 18), (u'esperanza', 18), (u'espacio', 18), (u'cincuenta', 18), (u'hablaba', 17), (u'Inglaterra', 17), (u'sentimiento', 17), (u'agreement', 17), (u'colegio', 17), (u'cuesti\\xf3n', 17), (u'dif\\xedcil', 17), (u'especial', 17), (u'pasaba', 17), (u'fisonom\\xeda', 17), (u'mismos', 17), (u'fortuna', 17), (u'cultura', 17), (u'existencia', 17), (u'viejos', 17), (u'compa\\xf1ero', 17), (u'maestro', 17), (u'precisamente', 17), (u'labios', 17), (u'combate', 17), (u'efecto', 17), (u'coronel', 17), (u'muchas', 17), (u'in\\xfatil', 17), (u'palabras', 17), (u'filosof\\xeda', 17), (u'peque\\xf1o', 17), (u'relaciones', 17), (u'Larsen', 16), (u'tienen', 16), (u'letras', 16), (u'Segovia', 16), (u'grande', 16), (u'alguna', 16), (u'seguida', 16), (u'opini\\xf3n', 16), (u'Galindo', 16), (u'quer\\xeda', 16), (u'claustros', 16), (u'fuerte', 16), (u'ilustre', 16), (u'partida', 16), (u'sistema', 16), (u'altura', 16), (u'respeto', 16), (u'lectura', 16), (u'absoluta', 15), (u'Tob\\xedas', 15), (u'materia', 15), (u'voluntad', 15), (u'recibido', 15), (u'objeto', 15), (u'juicio', 15), (u'ustedes', 15), (u'armon\\xeda', 15), (u'elementos', 15), (u'direcci\\xf3n', 15), (u'estudios', 15), (u'autoridad', 15), (u'Archive', 15), (u'ning\\xfan', 15), (u'partes', 15), (u'infancia', 15), (u'propia', 15), (u'figura', 15), (u'largas', 15), (u'argentino', 15), (u'argentina', 15), (u'peque\\xf1a', 15), (u'acuerdo', 15), (u'volver', 15), (u'respecto', 15), (u'hechos', 15), (u'l\\xedneas', 14), (u'mujeres', 14), (u'principio', 14), (u'imaginaci\\xf3n', 14), (u'art\\xedculos', 14), (u'\\xfaltimos', 14), (u'eterno', 14), (u'llamado', 14), (u'pueden', 14), (u'hab\\xedamos', 14), (u'horizonte', 14), (u'normal', 14), (u'estudiante', 14), (u'versos', 14), (u'delante', 14), (u'lentamente', 14), (u'caballos', 14), (u'Ferrari', 14), (u'reposo', 14), (u'fuente', 14), (u'condiciones', 14), (u'llegado', 14), (u'Mart\\xedn', 14), (u'entusiasmo', 13), (u'curioso', 13), (u't\\xedtulo', 13), (u'educaci\\xf3n', 13), (u'largos', 13), (u'Nacional', 13), (u'Literary', 13), (u'Vel\\xe1zquez', 13), (u'segundo', 13), (u'especie', 13), (u'pueblos', 13), (u'causas', 13), (u'principal', 13), (u'conquista', 13), (u'teatro', 13), (u'fen\\xf3meno', 13), (u'buenos', 13), (u'energ\\xeda', 13), (u'jabal\\xed', 13), (u'ciudad', 13), (u'ingl\\xe9s', 13), (u'Ateneo', 13), (u'brazos', 13), (u'visita', 13), (u'humano', 13), (u'diarios', 13), (u'atenci\\xf3n', 13), (u'Mozart', 13), (u'conoc\\xeda', 13), (u'desenvolvimiento', 13), (u'curiosidad', 13), (u'espa\\xf1oles', 13), (u'permitido', 13), (u'encontr\\xf3', 13), (u'suerte', 13), (u'cambio', 13), (u'conocer', 13), (u'experiencia', 13), (u'claustro', 13), (u'rostro', 13), (u'cierta', 13), (u'abierto', 13), (u'literatura', 13), (u'problema', 13), (u'escuela', 13), (u'centro', 13), (u'conocido', 13), (u'llegada', 13), (u'ligera', 12), (u'nuevos', 12), (u'sombrero', 12), (u'militar', 12), (u'p\\xe1gina', 12), (u'nuevas', 12), (u'profundo', 12), (u'padres', 12), (u'muerto', 12), (u'dejaba', 12), (u'gloria', 12), (u'empieza', 12), (u'comprender', 12), (u'Universidad', 12), (u'seguro', 12), (u'famoso', 12), (u'tendido', 12), (u'vuelta', 12), (u'querido', 12), (u'democracia', 12), (u'tomado', 12), (u'presente', 12), (u'tomaba', 12), (u'colosal', 12), (u'entrar', 12), (u'pintura', 12), (u'encontrar', 12), (u'estudiantes', 12), (u'secretario', 12), (u'extranjero', 12), (u'copyright', 12), (u'progreso', 12), (u'novela', 12), (u'terreno', 12), (u'quedado', 12), (u'donations', 12), (u'luchas', 12), (u'trav\\xe9s', 12), (u'p\\xfablico', 12), (u'desconocido', 12), (u'adelante', 12), (u'recursos', 11), (u'entrada', 11), (u'noches', 11), (u'Miguel', 11), (u'dignidad', 11), (u'pureza', 11), (u'necesidad', 11), (u'griego', 11), (u'indiferencia', 11), (u'sirviente', 11), (u'detr\\xe1s', 11), (u'inm\\xf3vil', 11), (u'silencioso', 11), (u'medios', 11), (u'Isidoro', 11), (u'f\\xedsica', 11), (u'esfuerzos', 11), (u'soldado', 11), (u'elegante', 11), (u'serena', 11), (u'pa\\xedses', 11), (u'pol\\xedtico', 11), (u'origen', 11), (u'escritor', 11), (u'incomparable', 11), (u'tenemos', 11), (u'cuadros', 11), (u'\\xfaltima', 11), (u'treinta', 11), (u'defensa', 11), (u'primeras', 11), (u'g\\xe9nero', 11), (u'fuerzas', 11), (u'simpat\\xeda', 11), (u'profundamente', 11), (u'Juvenilia', 10), (u'sentir', 10), (u'soberbio', 10), (u'tantos', 10), (u'formas', 10), (u'entero', 10), (u'ninguna', 10), (u'talento', 10), (u'bondad', 10), (u'triunfo', 10), (u'enormes', 10), (u'volvi\\xf3', 10), (u'buscar', 10), (u'prensa', 10), (u'animal', 10), (u'claridad', 10), (u'dejado', 10), (u'Presidente', 10), (u'escritores', 10), (u'Recuerdo', 10), (u'hiciera', 10), (u'Italia', 10), (u'ocupaba', 10), (u'italiano', 10), (u'metros', 10), (u'License', 10), (u'absurdo', 10), (u'recibi\\xf3', 10), (u'distancia', 10), (u'organizaci\\xf3n', 10), (u'noticia', 10), (u'llevaba', 10), (u'sinti\\xf3', 10), (u'tradici\\xf3n', 10), (u'miraba', 10), (u'presencia', 10), (u'vivido', 10), (u'organismo', 10), (u'conferencia', 10), (u'cuello', 10), (u'States', 10), (u'Adem\\xe1s', 10), (u'campana', 10), (u'gratitud', 10), (u'Caracas', 10), (u'paragraph', 10), (u'instituciones', 10), (u'confianza', 10), (u'Varela', 10), (u'piedra', 10), (u'belleza', 10), (u'access', 10), (u'concepci\\xf3n', 10), (u'cuarenta', 10), (u'verdadero', 10), (u'civilizaci\\xf3n', 10), (u'esperar', 10), (u'encuentro', 10), (u'Venezuela', 10), (u'caracteres', 10), (u'batalla', 10), (u'United', 10), (u'arriba', 10), (u'siglos', 10), (u'profesor', 10), (u'evoluci\\xf3n', 10), (u'escribir', 10), (u'sonrisa', 10), (u'asombro', 9)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fdist1 = FreqDist(text)\n",
    "print([i for i in fdist1.most_common(1000) if len(i[0]) > 5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
